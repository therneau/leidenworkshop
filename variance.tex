\documentclass{article}\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}[12pt]
\usepackage{amsmath}
\addtolength{\textwidth}{1in}
\addtolength{\oddsidemargin}{-.5in}
\setlength{\evensidemargin}{\oddsidemargin}
\newcommand{\Cvar}{{\cal I}^{-1}}
\newcommand{\code}[1]{\texttt{#1}}
\title{Variance in the survival package}
\author{Terry Therneau}
\date{30 Sept 2024}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\maketitle



\section{Infinitesimal jackknife}
If $\theta$ is a statistic that depends on a set of observations, then
\begin{align*}
  J_{(i)} &= (\theta - \theta_{-i}) \\
  V_J &= \frac{n-1}{n} \sum_i (J_{(i)} - \overline J)^2
\end{align*}
is the jackknife values and the jackknife estimate of variance,
respectively, where $-i$ denotes the estimate computed without observation
$i$.

The infinitesimal jackknife (IJ) values and IJ estimate of variance 
are defined as 
\begin{align*}
 \mathcal{J}_{(i)} &=  \left.\frac{\partial \beta_j}{\partial w_i}\right|_{w=1} \\
 V_{\mathcal{J}} &=   \sum \mathcal{J}^2_{(i)}
\end{align*}
 
An underlying idea is that 1 times the IJ is a first order Taylor series
approximation to the jackknife.  When $n>20$ and there are no large leverage
points, the approximation is often very good. 
Within the survival package, we can often compute the IJ in $O(n)$ time, versus
$O(n^2)$ for the jackknife.
One ancillary but useful property of the IJ values is that they sum to zero.
In terms of the sign, the survival package takes the view that if adding
observation $i$ increase the statistic, then the IJ leverage is positive.

Like many good statistical ideas the IJ has been rediscovered multiple times
in the statistical literature and appeared under multplie names.
Two common manifestations are White's standard
error for the linear model (or Huber-White, or Eicker-Huber-White), the
working independence variance estimate for a generalized estimating equation
(GEE) model, and the Horvitz-Thompson in survey sampling.
  Is is also called a sandwich estimate.

If there are multiple observations per subject then one can use the grouped
jackknife or grouped IJ, simply sum the values with group before adding
up the squares.  This is the default when a survfit or coxph call contain
an \code{id} or \code{cluster} statement.
In the more general case
let $\beta$ be a vector of $p$ parameters and $w$ the vector of $n$ case
weights, then the $n$ by $p$ dfbeta matrix $D$ is
\begin{align}
  D_{ij} &= \left.\frac{\partial \beta_j}{\partial w_i}\right|_{w=1} \\
  V &= D'WBW D  \label{ijvar}
\end{align}
where $W$ is the diagonal matrix of weights and
$B$ is ``the appropriate survey sample matrix'' (Binder).
In the case of groups, $B$ will be block diagonal where each block for a
group of size $m$ is an $m$ by $m$ matrix of 1s.  
For more general cases we refer readers to the survey package.

In the survival package, the \code{resid} function is slowly expanding to
deal with all the cases.
\begin{itemize}
  \item survfit:  \code{resid(fit, times=, type= cumhaz / pstate / sojourn)}
  \item coxph  :  \code{resid(fit, type='dfbeta')}
\end{itemize}
An optional collapse argument will return one row per id.
The function for a survival curve post MSH model has been in the ``any week now''
state for over a year.
For those who want to dig deeper, detailed formulas are found in the formula
appendix to my upcoming book.

The full IJ residual matrix for an Aalen-Johansen curve would have dimension
(number of observations) x (number of event times) x (number of states).
That from a post MSH curve will also have x (number of rows in newdata).
This can get very large, very fast, so choose your time points carefully.

Most estimands from a MSH can be based on the three primary outputs of hazard,
probability in state, or sojourn. 
This means that, at least in principle, a variance for an estimand $\theta$ 
can be based on the IJ values.
That is, first compute the influence of each observation on $\theta$ to get
an appropriate $D$ matrix, than apply the standard formula above.
How well this works in practice for various $\theta$ has not yet 
been investigated.

Computations to fit a MSH model are $O(2np) + O(dp^2)$ where $n$ is the number
of observations, $d$ the number of events and $p$ the number of coefficients.
Anything that is $O(n^2)$ or $O(nd)$ can be \emph{much} slower, so take care.
A growing `methods' vignette in the package has many pages devoted to the 
strategies I use to avoid this within the standard computations.
IJ residuals have been a challenge.
(SAS phreg has a leading term of $O(ndp)$ for (time, time2) data, BTW).

Notes:

1. For an ordinary KM, the IJ variance exactly equals the Greenwood variance.
This was a big surprise.  Ann Eaton was able to prove it by induction.

2. The AJ and post MSH curves are each a step function. 
The sojourn time in state $k$ = area under the $p_k(t)$ curve 
$$
\mathcal{J}_i(AUC_k(t)) = \sum_{s \le t} \mathcal{J}_i(p_k(s)) \Delta(s)$$
The naive algorithm that walks forward in time, udating each observation's
IJ at each event time, will be $O(nd)$ and thus may be very slow.  
How can we do this efficiently?

3. One way to think of the $D$ matrix for a Cox model is to consider a
Newton-Raphson update step near the solution.
At the solution the NR step is 0, per below, where $U$ is the vector of
first derivatives and $\cal I$ the information matrix.
Modifiy the weight for observation $i$ slightly and compute the change in
$\hat\beta$, this is line 2. 
Others have pointed out that the derivative of $\cal I$ is of smaller order than
the $dU$ term so we ignore it.
 Finally dU/dw is a matrix with
n rows, each containing the derivative of $U$ wrt weight $i$.

\begin{align*}
  0 &= U' \Cvar \\
  \frac{\partial \beta}{\partial w_i} &= \frac{\partial U}{\partial w_i} \Cvar +
  U \frac{\partial\Cvar}{\partial w_i} \\
  D & \approx \frac{dU}{dw} \Cvar \\
  D'D &\approx \Cvar \left[ \left(\frac{dU}{dw}\right)'\left(\frac{dU}{dw}
    \right) \right] \Cvar
\end{align*}

The final line has the form of a sandwich estimate.  

\section{Psuedovalues}
The IJ pseudovalue is
$$
 n \mathcal{J}_i + \hat\theta
$$
If there is no censoring, the IJ pseudovalue from a KM is $t_i$, the original
survival time.
For censored data the pseudovalue computed at time $s$ can act as an 
\emph{uncensored} representation of $\min(t_i, s)$ for each subject.
For instance

\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> sfit <- survfit(Surv(time, status) ~1, data=lung)
> ps <- pseudo(sfit, type="surv", time=365)  # 1 year survival
> hist(ps, nclass=25)
\end{verbatim}
\end{kframe}
\includegraphics[width=\maxwidth]{figures/psuedo-1} 
\begin{kframe}\begin{verbatim}
> 
> pfit <- glm(ps ~ ph.ecog + sex, family= gaussian(link= blogit()), lung)
> summary(pfit)

Call:
glm(formula = ps ~ ph.ecog + sex, family = gaussian(link = blogit()), 
    data = lung)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  -0.8458     0.4968  -1.703  0.09002
ph.ecog      -0.7243     0.2379  -3.045  0.00261
sex           0.8225     0.3195   2.574  0.01069

(Dispersion parameter for gaussian family taken to be 0.2749687)

    Null deviance: 66.545  on 226  degrees of freedom
Residual deviance: 61.590  on 224  degrees of freedom
  (1 observation deleted due to missingness)
AIC: 356.09

Number of Fisher Scoring iterations: 5
\end{verbatim}
\end{kframe}
\end{knitrout}

See the \code{psuedo} command in the survival package.
The pseudo library in R uses the standard jackknife, and is much slower.


\end{document}
